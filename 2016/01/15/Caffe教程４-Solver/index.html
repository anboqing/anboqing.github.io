<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Caffe教程４：Solver | 暗时间</title>
  <meta name="author" content="安勃卿">
  
  <meta name="description" content="Solver简介Solver执行模型的优化操作，它通过协调网络的前向推断和反向梯度传播来执行参数更新，来减少误差损失。学习职能被分配到solver和net上，sovler负责优化和产生参数更新，net负责计算loss和梯度。
Caffe包含了下面几个Solver：

Stochastic Gradi">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Caffe教程４：Solver"/>
  <meta property="og:site_name" content="暗时间"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">暗时间</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> Caffe教程４：Solver</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="Solver简介"><a href="#Solver简介" class="headerlink" title="Solver简介"></a>Solver简介</h2><p><code>Solver</code>执行模型的优化操作，它通过协调网络的前向推断和反向梯度传播来执行参数更新，来减少误差损失。学习职能被分配到<code>solver</code>和<code>net</code>上，<code>sovler</code>负责优化和产生参数更新，<code>net</code>负责计算loss和梯度。</p>
<p>Caffe包含了下面几个<code>Solver</code>：</p>
<ul>
<li>Stochastic Gradient Descent (type: “SGD”),</li>
<li>AdaDelta (type: “AdaDelta”),</li>
<li>Adaptive Gradient (type: “AdaGrad”),</li>
<li>Adam (type: “Adam”),</li>
<li>Nesterov’s Accelerated Gradient (type: “Nesterov”) and</li>
<li>RMSprop (type: “RMSProp”)</li>
</ul>
<p><code>Solver</code>的功能有：</p>
<ol>
<li>支撑整个优化过程的簿记工作，创建训练网络和测试网络。</li>
<li>通过调用forward/backward来迭代优化并更新参数。</li>
<li>周期性的用测试网络评估学习状态。</li>
<li>在优化过程中创建模型和<code>solver</code>的状态。</li>
</ol>
<p>每次迭代过程：</p>
<ol>
<li>调用网络的forward来计算输出和损失</li>
<li>调用网络的backward来计算梯度</li>
<li>根据solver的具体方法用梯度进行参数更新</li>
<li>根据学习率，历史记录和方法来更新solver的状态</li>
</ol>
<p>Solver把weights从初始化一直保持到模型学习结束。它也有CPU/GPU模式。</p>
<hr>
<h2 id="Solver的方法"><a href="#Solver的方法" class="headerlink" title="Solver的方法"></a>Solver的方法</h2><p>Solver的method处理通用的损失函数最小化的优化问题。对于数据集$D$,优化目标是整个数据集的$|D|$条数据的平均损失：<br>$$L(W) = \frac{1}{|D|} \sum_i^{|D|} f_W\left(X^{(i)}\right) + \lambda r(W)$$<br>其中$f_W\left(X^{(i)}\right)$是数据对象$X^{(i)}$的损失，而$r(W)$是一个正则化项，$\lambda$是学习率。实际中数据集的数量$|D|$可以很大，所以每个solver迭代过程我们都使用随机逼近目标的方法，抽取小批量数据 $N &lt;&lt;|D|$:</p>
<p>$$L(W) \approx \frac{1}{N} \sum_i^N f_W\left(X^{(i)}\right) + \lambda r(W)$$</p>
<p>模型在forward步骤中计算$f_W$然后在backward步骤中把梯度$\nabla f_W$反传。<br>参数更新$\Delta W$是由solver根据误差梯度$\nabla f_W$、正则化的梯度$\nabla r(W)$、和其他和优化方法相关的参数计算的。</p>
<hr>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p><strong>Stochastic gradient descent</strong>(type: “SGD”)随机梯度下降法按照负梯度$\nabla L(W)$的线型组合和前面的权重更新$V_t$更新权重$W$。<strong>学习率</strong> $\alpha$是负梯度的权重。<strong>动量</strong>$\mu$是前面一次更新的权重。</p>
<p>最终，我们用如下公式用当前的权重$W_t$和上一次更新值$V<em>t$来计算第$t+1$次迭代的更新值$V</em>{t+1}$和更新权重$W<em>{t+1}$:<br>$$V</em>{t+1} = \mu V_t - \alpha \nabla L(W<em>t)$$<br>$$W</em>{t+1} = W<em>t + V</em>{t+1}$$</p>
<p>学习“超参数”($\alpha$和$\mu$）可能需要一点微调技巧以便获取最佳结果。如果你不确定怎么开始的画，可以看看下面的黄金法则，了解更多的信息你可以参看Leon Bottou’s 的<a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf" target="_blank" rel="external">Stochastic Gradient Descent Tricks </a>.</p>
<p><strong>设置学习率$\alpha$和动量$\mu$的黄金法则</strong></p>
<p>在深度学习中使用SGD的一个好策略是把学习率$\alpha$设置为$\alpha \approx 0.01 = 10^{-2}$,然后用一个常量因子（例如１０）来在学习过程中调整它，使得损失达到一个明显的”平台”,重复这个过程几遍，通常，你可能想用一个动量$\mu = 0.9$或者相似的值。为了在迭代过程平滑权重更新，动量可以使SGD稳定且快速。<br>　　<br>　　<br>这个过程是 Krizhevsky[^1]在他们著名的ILSVRC-2012　CNN组冠军论文里采用的策略。Caffe让整个策略易于实现，就像我们在对<a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" target="_blank" rel="external">1</a>进行实现的时候做的那样(./examples/imagenet/alexnet_solver.prototxt)</p>
<p>为了这样使用学习率策略，你可以把下面这几行放在你的solver的prototxt文件里:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">base_lr: <span class="number">0.01</span>     <span class="meta"># begin training at a learning rate of 0.01 = 1e-2</span></div><div class="line"></div><div class="line">lr_policy: <span class="string">"step"</span> <span class="meta"># learning rate policy: drop the learning rate in <span class="meta-string">"steps"</span></span></div><div class="line">                  <span class="meta"># by a factor of gamma every stepsize iterations</span></div><div class="line"></div><div class="line">gamma: <span class="number">0.1</span>        <span class="meta"># drop the learning rate by a factor of 10</span></div><div class="line">                  # (i.e., multiply it by a factor of gamma = <span class="number">0.1</span>)</div><div class="line"></div><div class="line">stepsize: <span class="number">100000</span>  <span class="meta"># drop the learning rate every 100K iterations</span></div><div class="line"></div><div class="line">max_iter: <span class="number">350000</span>  <span class="meta"># train for 350K iterations total</span></div><div class="line"></div><div class="line">momentum: <span class="number">0.9</span></div></pre></td></tr></table></figure></p>
<p>在上面的设定中，我们总是使用<code>momentum</code>$\mu=0.9$。我们以基础学习率<code>base_lr</code>$\alpha = 0.01 = 10^{-2}$开始训练最早的100,000次迭代过程，然后将学习率乘以<code>gamma</code>$(\gamma)$,即用学习率 $\alpha’ = \alpha \gamma = (0.01) (0.1) = 0.001 = 10^{-3}$训练第100K~200K次迭代过程，然后用$\alpha’’ = 10^{-4}$来训练第200K~300K次迭代，最终用$\alpha’’’ = 10^{-5}$训练到第350K次迭代.</p>
<p>　　<br>注意动量在很多次迭代后，把$\mu$用因子$\frac{1}{1 - \mu}$乘以你更新的次数设置，所以你如果增加了$\mu$,那么最好相应的减少$\alpha$，反之亦然。<br>　　<br>　　<br>例如，设动量$\mu＝0.9$，我们得到一个有效的更新尺寸乘数$\frac{1}{1 - 0.9} = 10$,如果我们增加动量到$\mu=0.99$,那么我们就把更新大小乘数增加到了100,于是我们应该把学习率$\alpha$下调10.</p>
<p>注意到上面的设定仅仅是一个指导，他们肯定不能够保证是最优的，甚至都不能工作！如果学习过程误入歧途了（例如你看到很大或者NaN、inf等损失值或者输出），尝试着把<code>base_lr</code>减少，例如：减少到0.001，然后重训练，重复这个过程直到你找到一个合适的<code>base_lr</code>.</p>
<p>[^1]: A. Krizhevsky, I. Sutskever, and G. Hinton. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external"> ImageNet Classification with Deep Convolutional Neural Networks</a>. Advances in Neural Information Processing Systems, 2012.</p>
<hr>
<h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p><strong>AdaDelta</strong>方法(type: “AdaDelta”)是一个”robust learning rate method”.[^2].它也是一种基于梯度的优化方法（类似SGD)，它的更新公式是：</p>
<p>$$ \begin{align}<br>(v_t)<em>i &amp;= \frac{\operatorname{RMS}((v</em>{t-1})_i)}{\operatorname{RMS}\left( \nabla L(W<em>t) \right)</em>{i}} \left( \nabla L(W_{t’}) \right)_i<br>\<br>\operatorname{RMS}\left( \nabla L(W<em>t) \right)</em>{i} &amp;= \sqrt{E[g^2] + \varepsilon}<br>\<br>E[g^2]<em>t &amp;= \delta{E[g^2]</em>{t-1} } + (1-\delta)g<em>{t}^2<br>\end{align} $$<br>$$(W</em>{t+1})_i =<br>(W_t)_i - \alpha<br>(v_t)_i.$$</p>
<hr>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p><strong>adaptive gradient</strong>(type: “AdaGrad”)方法（Duchi, E. Hazan, and Y. Singer.<a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" target="_blank" rel="external"> Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> The Journal of Machine Learning Research, 2011.）<br>是一个基于梯度的方法，它尝试“在干草堆里找缝衣针”也就是找到预测能力很强但很少见的feature（find needles in haystacks in the form of very predictive but rarely seen features,）,Duchi等人如是说。给出前面迭代的所有更新信息($\left( \nabla L(W) \right)_{t’}$其中($t’ \in {1, 2, …, t}$),论文中提出的更新公式如下，它给每个组件$i$指定一个权重W:</p>
<p>$$<br>(W_{t+1})_i =<br>(W_t)_i - \alpha<br>\frac{\left( \nabla L(W<em>t) \right)</em>{i}}{<br>    \sqrt{\sum<em>{t’=1}^{t} \left( \nabla L(W</em>{t’}) \right)_i^2}<br>}<br>$$</p>
<p>注意在实践中，对于权重$W \in \mathcal{R}^d$,AdaGrad实现仅用了$\mathcal{O}(d)$额外的存储来保存历史梯度信息，而传统方法要用$\mathcal{O}(dt)$来保存历史梯度信息。</p>
<hr>
<h2 id="Scaffolding"><a href="#Scaffolding" class="headerlink" title="Scaffolding"></a>Scaffolding</h2><p>Solver的scaffolding准备优化方法并且初始化模型，通过调用<code>Solver::Presolve()</code>方法调用来完成。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt; caffe train -solver examples/mnist/lenet_solver.prototxt</div><div class="line">I0902 13:35:56.474978 16020 caffe.cpp:90] Starting Optimization</div><div class="line">I0902 13:35:56.475190 16020 solver.cpp:32] Initializing solver from parameters:</div><div class="line">test_iter: 100</div><div class="line">test_interval: 500</div><div class="line">base_lr: 0.01</div><div class="line">display: 100</div><div class="line">max_iter: 10000</div><div class="line">lr_policy: &quot;inv&quot;</div><div class="line">gamma: 0.0001</div><div class="line">power: 0.75</div><div class="line">momentum: 0.9</div><div class="line">weight_decay: 0.0005</div><div class="line">snapshot: 5000</div><div class="line">snapshot_prefix: &quot;examples/mnist/lenet&quot;</div><div class="line">solver_mode: GPU</div><div class="line">net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</div></pre></td></tr></table></figure>
<p>网络初始化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">I0902 13:35:56.655681 16020 solver.cpp:72] Creating training net from net file: examples/mnist/lenet_train_test.prototxt</div><div class="line">[...]</div><div class="line">I0902 13:35:56.656740 16020 net.cpp:56] Memory required for data: 0</div><div class="line">I0902 13:35:56.656791 16020 net.cpp:67] Creating Layer mnist</div><div class="line">I0902 13:35:56.656811 16020 net.cpp:356] mnist -&gt; data</div><div class="line">I0902 13:35:56.656846 16020 net.cpp:356] mnist -&gt; label</div><div class="line">I0902 13:35:56.656874 16020 net.cpp:96] Setting up mnist</div><div class="line">I0902 13:35:56.694052 16020 data_layer.cpp:135] Opening lmdb examples/mnist/mnist_train_lmdb</div><div class="line">I0902 13:35:56.701062 16020 data_layer.cpp:195] output data size: 64,1,28,28</div><div class="line">I0902 13:35:56.701146 16020 data_layer.cpp:236] Initializing prefetch</div><div class="line">I0902 13:35:56.701196 16020 data_layer.cpp:238] Prefetch initialized.</div><div class="line">I0902 13:35:56.701212 16020 net.cpp:103] Top shape: 64 1 28 28 (50176)</div><div class="line">I0902 13:35:56.701230 16020 net.cpp:103] Top shape: 64 1 1 1 (64)</div><div class="line">[...]</div><div class="line">I0902 13:35:56.703737 16020 net.cpp:67] Creating Layer ip1</div><div class="line">I0902 13:35:56.703753 16020 net.cpp:394] ip1 &lt;- pool2</div><div class="line">I0902 13:35:56.703778 16020 net.cpp:356] ip1 -&gt; ip1</div><div class="line">I0902 13:35:56.703797 16020 net.cpp:96] Setting up ip1</div><div class="line">I0902 13:35:56.728127 16020 net.cpp:103] Top shape: 64 500 1 1 (32000)</div><div class="line">I0902 13:35:56.728142 16020 net.cpp:113] Memory required for data: 5039360</div><div class="line">I0902 13:35:56.728175 16020 net.cpp:67] Creating Layer relu1</div><div class="line">I0902 13:35:56.728194 16020 net.cpp:394] relu1 &lt;- ip1</div><div class="line">I0902 13:35:56.728219 16020 net.cpp:345] relu1 -&gt; ip1 (in-place)</div><div class="line">I0902 13:35:56.728240 16020 net.cpp:96] Setting up relu1</div><div class="line">I0902 13:35:56.728256 16020 net.cpp:103] Top shape: 64 500 1 1 (32000)</div><div class="line">I0902 13:35:56.728270 16020 net.cpp:113] Memory required for data: 5167360</div><div class="line">I0902 13:35:56.728287 16020 net.cpp:67] Creating Layer ip2</div><div class="line">I0902 13:35:56.728304 16020 net.cpp:394] ip2 &lt;- ip1</div><div class="line">I0902 13:35:56.728333 16020 net.cpp:356] ip2 -&gt; ip2</div><div class="line">I0902 13:35:56.728356 16020 net.cpp:96] Setting up ip2</div><div class="line">I0902 13:35:56.728690 16020 net.cpp:103] Top shape: 64 10 1 1 (640)</div><div class="line">I0902 13:35:56.728705 16020 net.cpp:113] Memory required for data: 5169920</div><div class="line">I0902 13:35:56.728734 16020 net.cpp:67] Creating Layer loss</div><div class="line">I0902 13:35:56.728747 16020 net.cpp:394] loss &lt;- ip2</div><div class="line">I0902 13:35:56.728767 16020 net.cpp:394] loss &lt;- label</div><div class="line">I0902 13:35:56.728786 16020 net.cpp:356] loss -&gt; loss</div><div class="line">I0902 13:35:56.728811 16020 net.cpp:96] Setting up loss</div><div class="line">I0902 13:35:56.728837 16020 net.cpp:103] Top shape: 1 1 1 1 (1)</div><div class="line">I0902 13:35:56.728849 16020 net.cpp:109]     with loss weight 1</div><div class="line">I0902 13:35:56.728878 16020 net.cpp:113] Memory required for data: 5169924</div></pre></td></tr></table></figure></p>
<p>损失函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">I0902 13:35:56.728893 16020 net.cpp:170] loss needs backward computation.</div><div class="line">I0902 13:35:56.728909 16020 net.cpp:170] ip2 needs backward computation.</div><div class="line">I0902 13:35:56.728924 16020 net.cpp:170] relu1 needs backward computation.</div><div class="line">I0902 13:35:56.728938 16020 net.cpp:170] ip1 needs backward computation.</div><div class="line">I0902 13:35:56.728953 16020 net.cpp:170] pool2 needs backward computation.</div><div class="line">I0902 13:35:56.728970 16020 net.cpp:170] conv2 needs backward computation.</div><div class="line">I0902 13:35:56.728984 16020 net.cpp:170] pool1 needs backward computation.</div><div class="line">I0902 13:35:56.728998 16020 net.cpp:170] conv1 needs backward computation.</div><div class="line">I0902 13:35:56.729014 16020 net.cpp:172] mnist does not need backward computation.</div><div class="line">I0902 13:35:56.729027 16020 net.cpp:208] This network produces output loss</div><div class="line">I0902 13:35:56.729053 16020 net.cpp:467] Collecting Learning Rate and Weight Decay.</div><div class="line">I0902 13:35:56.729071 16020 net.cpp:219] Network initialization done.</div><div class="line">I0902 13:35:56.729085 16020 net.cpp:220] Memory required for data: 5169924</div><div class="line">I0902 13:35:56.729277 16020 solver.cpp:156] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt</div></pre></td></tr></table></figure></p>
<p>完成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">I0902 13:35:56.806970 16020 solver.cpp:46] Solver scaffolding done.</div><div class="line">I0902 13:35:56.806984 16020 solver.cpp:165] Solving LeNet</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><p>实际的权重更新是由solver完成的，它通过把参数传递给<code>Sovler::ComputeUpdateValue()</code>来实现的。这个方法会整合所有权重衰减到权重梯度里（现在只包含误差梯度）来获得每个网络最终的梯度,然后这些gradients被学习率$\alpha$缩放，需要减去的更新值被存在每个参数<code>Blob</code>的<code>diff</code>属性里。最终，调用每个参数blob的<code>Blob::Update</code>方法,完成最终的权重更新（从它的<code>data</code>里减去<code>diff</code>）.</p>
<hr>
<h2 id="快照和恢复"><a href="#快照和恢复" class="headerlink" title="快照和恢复"></a>快照和恢复</h2><p>Solvor在训练过程中通过调用<code>Solver::Snapshot()</code>方法和<code>Solver::SnapshotSolverState()</code>方法分别保存权重和它自身的状态。权重快照导出了学习到的模型，solver快照相当一个断点，使得训练可以从该快照恢复。恢复是通过<code>Solver::Restore()</code>和<code>Solver::RestoreSolverState()</code>方法完成的.</p>
<p>权重保存文件没有后缀，而状态保存文件有<code>.solverstate</code>的后缀。每个文件都会有一个<code>_iter_N</code>的前缀来指明是多少次迭代的快照。</p>
<p>快照是在Solver定义prototxt里这样配置的:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">The snapshot interval in iterations.</div><div class="line">snapshot: <span class="number">5000</span></div><div class="line"># File path prefix <span class="keyword">for</span> snapshotting model weights and solver state.</div><div class="line"># Note: <span class="keyword">this</span> is relative to the invocation of the `caffe` utility, not the</div><div class="line"><span class="meta"># solver definition file.</span></div><div class="line">snapshot_prefix: <span class="string">"/path/to/model"</span></div><div class="line"># Snapshot the diff along with the weights. This can help debugging training</div><div class="line"><span class="meta"># but takes more storage.</span></div><div class="line">snapshot_diff: <span class="literal">false</span></div><div class="line"># A final snapshot is saved at the end of training unless</div><div class="line"><span class="meta"># this flag is set to false. The default is true.</span></div><div class="line">snapshot_after_train: <span class="literal">true</span></div></pre></td></tr></table></figure></p>
<p>[^2]: M. Zeiler ADADELTA: <a href="http://arxiv.org/pdf/1212.5701.pdf" target="_blank" rel="external">AN ADAPTIVE LEARNING RATE METHOD</a>. arXiv preprint, 2012.</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2016/05/12/由补码引发的关于编码的思考/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2016/01/14/损失函数/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2016-01-15 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Caffe教程/">Caffe教程<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/Caffe教程/">Caffe教程<span>4</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2016 安勃卿
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  <a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
   </html>
