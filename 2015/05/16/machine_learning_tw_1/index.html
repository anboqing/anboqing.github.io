<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>机器学习概念介绍 | 暗时间</title>
  <meta name="author" content="安勃卿">
  
  <meta name="description" content="[TOC]
机器学习如何学习机器学习是一门理论和实践相结合的科目

理论导向
derive everything deeply for solid understanding
不关心普罗大众


技术导向

flash over the sexiest techniques broadly for ">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="机器学习概念介绍"/>
  <meta property="og:site_name" content="暗时间"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">暗时间</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 机器学习概念介绍</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>[TOC]</p>
<h2 id="机器学习如何学习"><a href="#机器学习如何学习" class="headerlink" title="机器学习如何学习"></a>机器学习如何学习</h2><p><strong>机器学习是一门理论和实践相结合的科目</strong></p>
<ul>
<li>理论导向<ul>
<li>derive everything <strong>deeply</strong> for solid understanding</li>
<li>不关心普罗大众</li>
</ul>
</li>
<li><p>技术导向</p>
<ul>
<li>flash over the sexiest techniques <strong>broadly</strong> for shiny coverage.</li>
<li>有太多技术，很难选择，很难正确的使用到合适的场景</li>
</ul>
<p>所以纯粹死学理论和不钻研理论只学花哨的技术都不合适，我们要从<strong>机器学习的本质基础</strong>学起。</p>
<ul>
<li>要学习关键理论，关键技术，实践中的用法</li>
<li>学习每个机器学习者都必须掌握的东西</li>
</ul>
</li>
</ul>
<p><strong>从学习到机器学习</strong></p>
<ul>
<li><p><strong>学习：</strong>acquiring <strong>skill</strong> with experience accumulated from <strong>abservations</strong> 从观察积累的知识出发学习技能</p>
<blockquote>
<p>观察  —&gt;  Learning —-&gt; skill</p>
</blockquote>
</li>
<li><p><strong>机器学习</strong>：aquiring <strong>skill</strong> with experience accumulated/<strong>computed</strong> from <strong>data</strong>;</p>
<blockquote>
<p>data —-&gt;  ML  ——&gt; skill</p>
</blockquote>
</li>
<li><p>Skill : improve some <strong>performance measure</strong> ( e.g. prediction accuracy)</p>
<blockquote>
<p>data —-&gt; ML —–&gt; improved performance measure<br>机器学习就是从数据出发，学习到技巧，从而对现有方案有促进提高</p>
</blockquote>
</li>
<li><p><strong>机器可学习的场景</strong></p>
<ul>
<li>存在需要被学习的模式</li>
<li>但是没有可以很容易的对模式定义的数学描述</li>
<li>并且存在关于该模式的大量数据 </li>
</ul>
</li>
</ul>
<hr>
<h2 id="机器学习的例子"><a href="#机器学习的例子" class="headerlink" title="机器学习的例子"></a>机器学习的例子</h2><ol>
<li>food：从社交网络上挖掘文本和位置信息，学习餐厅的卫生状况对健康的影响</li>
<li>Clothing: 用销售数据和客户调查数据来给客户进行穿衣搭配的推荐</li>
<li>Housing: 从建筑特征和能耗负载数据来预测建筑的能耗</li>
<li>行：自动驾驶</li>
</ol>
<hr>
<h2 id="机器学习和其他领域的关系"><a href="#机器学习和其他领域的关系" class="headerlink" title="机器学习和其他领域的关系"></a>机器学习和其他领域的关系</h2><h3 id="机器学习和数据挖掘"><a href="#机器学习和数据挖掘" class="headerlink" title="机器学习和数据挖掘"></a>机器学习和数据挖掘</h3><ul>
<li>机器学习：用数据去算出一个和目标函数很接近的假设函数 </li>
<li>数据挖掘：用大量数据去找到数据里面有用有趣的性质，关联等<ul>
<li>如果把数据挖掘的目标限制为<strong>找到一个和目标函数很接近的假设函数</strong>的话，那么机器学习和数据挖掘没什么本质的不同，他们目标是一致的。</li>
<li>但是数据挖掘的目标并不总是这样，如果interesting property和’hypothesis that approximate target是相关的，那么 数据挖掘 可以帮助机器学习，并且反过来也一样（vice versa)</li>
<li>传统的数据挖掘同样也关注在大数据库里实现高效的计算</li>
</ul>
</li>
<li>他们非常相像，但是不完全相同</li>
</ul>
<hr>
<h3 id="机器学习和人工智能"><a href="#机器学习和人工智能" class="headerlink" title="机器学习和人工智能"></a>机器学习和人工智能</h3><ul>
<li>机器学习： use data to compute hypothesis g that approximates target f</li>
<li>人工智能：compute something <strong>that shows intelligent behavior</strong>。<pre><code>* 如果把机器学习要学习的目标函数的功能限定为，这个函数可以让计算机实现智能化的行为（自动驾驶），那么机器学习和人工智能就是相同的。
* 但是实现智能的途径不只有机器学习一种。
</code></pre></li>
</ul>
<hr>
<h3 id="机器学习和统计学"><a href="#机器学习和统计学" class="headerlink" title="机器学习和统计学"></a>机器学习和统计学</h3><ul>
<li>Statistics : use data to <strong>make inference about an unknown process</strong></li>
<li>g is an inference outcome; f is something unknown ; statistics <strong>can be used to achieve ML</strong></li>
<li>传统的统计学同样也专注于证明数学假设，但是不关心如何计算</li>
<li>机器学习用到的许多工具很早就在统计学里面出现了，所以统计学为机器学习提供了有力的工具</li>
</ul>
<hr>
<h2 id="机器学习的组成部分"><a href="#机器学习的组成部分" class="headerlink" title="机器学习的组成部分"></a>机器学习的组成部分</h2><p><strong>基本符号</strong></p>
<ul>
<li>input: x$\in$X </li>
<li>output: y $\in$ Y</li>
<li><strong>unknown pattern to be learned $\Leftarrow\Rightarrow$</strong> <strong>target function</strong>:  f: X$\rightarrow$Y</li>
<li>data $\Leftarrow\Rightarrow$ training examples</li>
<li>hypothesis $\Leftarrow\Rightarrow$ skill with hopefully good performance: g: X $\rightarrow$ Y<br><img src="http://7xia5s.com1.z0.glb.clouddn.com/ml_ml_1_1.png" alt="ml_1_1"></li>
</ul>
<hr>
<p>##参考资料<br>經典文獻<br>F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386-408, 1958. (第二講：Perceptron 的出處)      </p>
<p>W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963. (第四講：Hoeffding’s Inequality)    </p>
<p>Y. S. Abu-Mostafa, X. Song , A. Nicholson, M. Magdon-ismail. The bin model, 1995. (第四講：bin model 的出處)</p>
<p>V. Vapnik. The nature of statistical learning theory, 2nd edition, 2000. (第五到八講：VC dimension 與 VC bound 的完整數學推導及延伸)</p>
<p>Y. S. Abu-Mostafa. The Vapnik-Chervonenkis dimension: information versus complexity in learning. Neural Computation, 1(3):312-317, 1989. (第七講：VC Dimension 的概念與重要性)</p>
<p>參考文獻<br>A. Sadilek, S. Brennan, H. Kautz, and V. Silenzio. nEmesis: Which restaurants should you avoid today? First AAAI Conference on Human Computation and Crowdsourcing, 2013. (第一講：ML 在「食」的應用)</p>
<p>Y. S. Abu-Mostafa. Machines that think for themselves. Scientific American, 289(7):78-81, 2012. (第一講：ML 在「衣」的應用)</p>
<p>A. Tsanas, A. Xifara. Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools. Energy and Buildings, 49: 560-567, 2012. (第一講：ML 在「住」的應用)</p>
<p>J. Stallkamp, M. Schlipsing, J. Salmen, C. Igel. Introduction to the special issue on machine learning for traffic sign recognition. IEEE Transactions on Intelligent Transportation Systems 13(4): 1481-1483, 2012. (第一講：ML 在「行」的應用)</p>
<p>R. Bell, J. Bennett, Y. Koren, and C. Volinsky. The million dollar programming prize. IEEE Spectrum, 46(5):29-33, 2009. (第一講：Netflix 大賽)</p>
<p>S. I. Gallant. Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, 1(2):179-191, 1990. (第二講：pocket 的出處，注意到實際的 pocket 演算法比我們介紹的要複雜)   </p>
<p>R. Xu, D. Wunsch II. Survey of clustering algorithms. IEEE Transactions on Neural Networks 16(3), 645-678, 2005. (第三講：Clustering)</p>
<p>X. Zhu. Semi-supervised learning literature survey. University of Wisconsin Madison, 2008. (第三講：Semi-supervised)</p>
<p>Z. Ghahramani. Unsupervised learning. In Advanced Lectures in Machine Learning (MLSS ’03), pages 72–112, 2004. (第三講：Unsupervised)</p>
<p>L. Kaelbling, M. Littman, A. Moore. reinforcement learning: a survey. Journal of Artificial Intelligence Research, 4: 237-285. (第三講：Reinforcement)</p>
<p>A. Blum. On-Line algorithms in machine learning. Carnegie Mellon University,1998. (第三講：Online)</p>
<p>B. Settles. Active learning literature survey. University of Wisconsin Madison, 2010. (第三講：Active)</p>
<p>D. Wolpert. The lack of a priori distinctions between learning algorithms. Neural Computation, 8(7): 1341-1390. (第四講：No free lunch 的正式版)</p>
<p>T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic Computers, 14(3):326–334, 1965. (第五到六講：Growth Function)</p>
<p>B. Zadrozny, J. Langford, N. Abe. Cost sensitive learning by cost-proportionate example weighting. IEEE International Conference on Data Mining, 2003. (第八講：Weighted Classification)</p>
<p>G. Sever, A. Lee. Linear Regression Analysis, 2nd Edition, Wiley, 2003. (第九講：Linear Regression </p>
<pre><code>由統計學的角度來分析；第十二到十三講：Polynomial Transform 後再做 Linear Regression)
</code></pre><p>D. C. Hoaglin, R. E. Welsch. The hat matrix in regression and ANOVA. American Statistician, 32:17–22, 1978. (第九講：Linear Regression 的 Hat Matrix)</p>
<p>D. W. Hosmer, Jr., S. Lemeshow, R. X. Sturdivant. Applied Logistic Regression, 3rd Edition, Wiley, 2013 (第十講：Logistic Regression 由統計學的角度來分析)</p>
<p>T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. International Conference on Machine Learning,  (第十一講：Stochastic Gradient Descent 用在線性模型的理論分析)</p>
<p>R. Rifkin, A. Klautau. In Defense of One-Vs-All Classification. Journal of Machine Learning Research, 5: 101-141, 2004. (第十一講：One-versus-all)</p>
<p>J. Fürnkranz. Round Robin Classification. Journal of Machine Learning Research, 2: 721-747, 2002. (第十一講：One-versus-one)</p>
<p>L. Li, H.-T. Lin. Optimizing 0/1 loss for perceptrons by random coordinate descent. In Proceedings of the 2007 International Joint Conference on Neural Networks (IJCNN ’07), pages 749–754, 2007. (第十一講：一個由最佳化角度出發的 Perceptron Algorithm)<br>G.-X. Yuan, C.-H. Ho, C.-J. Lin. Recent advances of large-scale linear classification. Proceedings of IEEE, 2012. (第十一講：更先進的線性分類方法)</p>
<p>Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, C.-J. Lin. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research, 11(2010), 1471-1490. (第十二講：一個使用多項式轉換加上線性分類模型的方法)<br>M. Magdon-Ismail, A. Nicholson, Y. S. Abu-Mostafa. Learning in the presence of noise. In Intelligent Signal Processing. IEEE Press, 2001. (第十三講：Noise 和 Learning)</p>
<p>A. Neumaier, Solving ill-conditioned and singular linear systems: A tutorial on regularization, SIAM Review 40 (1998), 636-666. (第十四講：Regularization)</p>
<p>T. Poggio, S. Smale. The mathematics of learning: Dealing with data. Notices of the American Mathematical Society, 50(5):537–544, 2003. (第十四講：Regularization)</p>
<p>P. Burman. A comparative study of ordinary cross-validation, v-fold cross-validation and the repeated learning-testing methods. Biometrika, 76(3): 503–514, 1989. (第十五講：Cross Validation)</p>
<p>R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the 14th International Joint Conference on Artificial intelligence (IJCAI ’95), volume 2, 1137–1143, 1995. (第十五講：Cross Validation)<br>A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occam’s razor. Information Processing Letters, 24(6):377–380, 1987. (第十六講：Occam’s Razor)</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2015/05/18/machine_learning_tw_2/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2015/05/13/pattern_discovery_2/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2015-05-16 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/机器学习/">机器学习<span>6</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/机器学习/">机器学习<span>3</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2016 安勃卿
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  <a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
   </html>
